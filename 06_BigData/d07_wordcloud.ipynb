{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from wordcloud import STOPWORDS, WordCloud\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_csv('./csv/riss_bigdata.csv', encoding='utf-8')\n",
    "all_title = all_data['제목']\n",
    "all_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = set(stopwords.words('english'))\n",
    "lemma = WordNetLemmatizer()\n",
    "words = []\n",
    "\n",
    "for title in all_title:\n",
    "    EnWords = re.sub(r'[^A-Za-z]',' ', str(title))\n",
    "    EnWordsToken = word_tokenize(EnWords.lower())\n",
    "    EnWordsTokenStop = [w for w in EnWordsToken if w not in stopWords]\n",
    "    EnWordsTokenStopLemma = [lemma.lemmatize(w) for w in EnWordsTokenStop]\n",
    "    words.append(EnWordsTokenStopLemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "words2 = list(reduce(lambda x, y : x+y, words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "count = Counter(words2)\n",
    "\n",
    "word_count = dict()\n",
    "\n",
    "for tag, counts in count.most_common(50):\n",
    "    if( len(str(tag))> 1):\n",
    "        word_count[tag] = counts\n",
    "\n",
    "word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sorted_Keys = sorted(word_count, key = word_count.get, reverse=True)\n",
    "sorted_Values = sorted(word_count.values(), reverse=True)\n",
    "\n",
    "plt.bar(range(len(word_count)), sorted_Values, align='center')\n",
    "plt.xticks(range(len(word_count)), list(sorted_Keys), rotation = '85')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del word_count['big']\n",
    "del word_count['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strpwords = set(STOPWORDS)\n",
    "wc = WordCloud(background_color='ivory', stopwords=stopwords, width=800, height=600)\n",
    "cloud = wc.generate_from_frequencies(word_count)\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(cloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "import pytagcloud\n",
    "\n",
    "\n",
    "f = open('data.txt', encoding='utf-8')\n",
    "data = f.read()\n",
    "f.close()\n",
    "\n",
    "nlp = Okt()\n",
    "nouns = nlp.nouns(data)\n",
    "count = Counter(nouns)\n",
    "tag2 = count.most_common(10)\n",
    "tag2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taglist = pytagcloud.make_tags(tag2, maxsize=50)\n",
    "pytagcloud.create_tag_image(taglist,\n",
    "                            'pytag_word.png',\n",
    "                            size=(640,480),\n",
    "                            fontname='Korean',\n",
    "                            rectangular=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as req\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import pandas as pd\n",
    "\n",
    "# getVillageWaterQuality , getGugunDong\n",
    "def call_getVillageWaterQuality():\n",
    "    url = 'http://apis.data.go.kr/6260000/TownWaterQualityService/getVillageWaterQuality'\n",
    "    params ={'serviceKey' : 'l9KDvSgYZZPF7NGBPKjJgV6/pU+fhBakLsJSW1avERVypdf2WAD6J4uaF5XHRhOYdZnLhUFU/XFa5W8z+5wGqw==', \n",
    "            'pageNo' : '1', \n",
    "            'numOfRows' : '745',\n",
    "            'resultType' : 'json', \n",
    "            # 'argGugun' : '710', \n",
    "            # 'argHjd' : '330' \n",
    "            }\n",
    "\n",
    "    res = req.get(url, params=params)\n",
    "    # print(res.json())\n",
    "    \n",
    "    global df_quailty\n",
    "    df_quailty = pd.DataFrame(res.json()['getVillageWaterQuality']['item'])\n",
    "    df_quailty.rename(columns = {'argGugun' : 'siguCd', 'argHjd' : 'dongCd'}, inplace = True)\n",
    "    df_quailty.to_csv('./csv/busanwater_nums.csv', encoding='utf-8-sig')\n",
    "    # {'getVillageWaterQuality': \n",
    "    #     {'header': {'code': '00', 'message': 'NORMAL_CODE'}, \n",
    "    #      'item': [{'orgAreaNm': '명장정수장', 'gb': '정수장', 'ph': '6.79', 'td': '점검중', 'jy': '0.62', 'ec': '-', 'tp': '-', 'sopCheck': '점검중', 'argGugun': '710', 'argHjd': '330'}, \n",
    "    #               {'orgAreaNm': '고촌배수지', 'gb': '배수지', 'ph': '7.06', 'td': '0.04', 'jy': '0.16', 'ec': '176.16', 'tp': '24.10', 'sopCheck': '정상', 'argGugun': '710', 'argHjd': '330'}, \n",
    "    #               {'orgAreaNm': '정관(택지)고지배수지', 'gb': '배수지', 'ph': '7.11', 'td': '0.09', 'jy': '0.20', 'ec': '-', 'tp': '-', 'sopCheck': '정상', 'argGugun': '710', 'argHjd': '330'}], \n",
    "    #      'numOfRows': 3, \n",
    "    #      'pageNo': 1, \n",
    "    #      'totalCount': 4}\n",
    "    # }\n",
    "\n",
    "def call_getGugunDong():\n",
    "    url = 'http://apis.data.go.kr/6260000/TownWaterQualityService/getGugunDong'\n",
    "    params ={'serviceKey' : 'l9KDvSgYZZPF7NGBPKjJgV6/pU+fhBakLsJSW1avERVypdf2WAD6J4uaF5XHRhOYdZnLhUFU/XFa5W8z+5wGqw==', \n",
    "            'pageNo' : '1', \n",
    "            'numOfRows' : '252',\n",
    "            'resultType' : 'json'\n",
    "            }\n",
    "    \n",
    "    \n",
    "    res = req.get(url, params=params)\n",
    "    # {'getGugunDong': \n",
    "    #     {'header': {'code': '00', 'message': 'NORMAL_CODE'}, \n",
    "    #      'item': [{'siguCd': '710', 'siguNm': '기장군', 'dongCd': '330', 'dongNm': '철마면'}], \n",
    "    #      'numOfRows': 1, \n",
    "    #      'pageNo': 1, \n",
    "    #      'totalCount': 252}\n",
    "    # } \n",
    "\n",
    "    global df_gundong, dongNm\n",
    "    df_gundong = pd.DataFrame(res.json()['getGugunDong']['item'])\n",
    "    dongNm = df_gundong['dongNm']\n",
    "    # print(len(set(dongNm)))\n",
    "    df_gundong.to_csv('./csv/busanwater.csv', encoding='utf-8-sig')\n",
    "\n",
    "\n",
    "call_getVillageWaterQuality()\n",
    "call_getGugunDong()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict_values'>\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# df_gundong , df_quailty\n",
    "df_concat = pd.merge(df_gundong ,df_quailty, on = [\"dongCd\",\"siguCd\"])\n",
    "# print(set(dongNm) - set(df_concat['dongNm']))\n",
    "# df_concat.to_csv('./csv/busan_water_total.csv', encoding='utf-8-sig')\n",
    "# print(df_concat.to_dict('index'))\n",
    "water = df_concat.to_dict('index')\n",
    "print(type(water.values()))\n",
    "# print(type(water))\n",
    "for i in list(water)[:10]:\n",
    "    # print(water[i])\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "610a6f344c2137faf927ea819c63f6cee33a2c04455044b28099f39fe9722347"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
